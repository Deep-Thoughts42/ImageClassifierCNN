{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 36\n",
    "valid_size = 0.2\n",
    "\n",
    "# Data augmentation for train data + conversion to tensor\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(12),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "   \n",
    "])\n",
    "\n",
    "\n",
    "# Data augmentation for test data + conversion to tensor\n",
    "test_transforms= transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# Picking Fashion-MNIST dataset\n",
    "train_data = datasets.FashionMNIST('Dataset', train=True, download=True, transform=train_transforms)\n",
    "test_data = datasets.FashionMNIST('Dataset', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "# Finding indices for validation set\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "#Randomize indices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(num_train*valid_size))\n",
    "train_index, test_index = indices[split:], indices[:split]\n",
    "\n",
    "# Making samplers for training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(test_index)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# Image classes\n",
    "classes = ['T-shirt/top','Trouser','Pullover','Dress',\n",
    "           'Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imshow Taken from Udacity Intro to Machine Learning Course\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "        \n",
    "    image = image.transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "\n",
    "imshow(images[0,:])\n",
    "print(classes[labels[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Will train model on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\user\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:132: UserWarning: \n",
      "    Found GPU0 GeForce GTX 760 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding =1)\n",
    "        # linear layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # max pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convolutional layers with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flattening the image\n",
    "        x = x.view(-1, 7*7*16)\n",
    "        # linear layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        \n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    print('Will train model on GPU')\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# loss function (cross entropy loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:698: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "Training Loss: 2.303327\n",
      "Validation Loss: 2.301204\n",
      "Validation loss decreased from inf to 2.301204\n",
      "Current Epoch: 2\n",
      "Training Loss: 2.299417\n",
      "Validation Loss: 2.296374\n",
      "Validation loss decreased from 2.301204 to 2.296374\n",
      "Current Epoch: 3\n",
      "Training Loss: 2.294641\n",
      "Validation Loss: 2.290173\n",
      "Validation loss decreased from 2.296374 to 2.290173\n",
      "Current Epoch: 4\n",
      "Training Loss: 2.28691\n",
      "Validation Loss: 2.278475\n",
      "Validation loss decreased from 2.290173 to 2.278475\n",
      "Current Epoch: 5\n",
      "Training Loss: 2.271698\n",
      "Validation Loss: 2.25325\n",
      "Validation loss decreased from 2.278475 to 2.25325\n",
      "Current Epoch: 6\n",
      "Training Loss: 2.231725\n",
      "Validation Loss: 2.175184\n",
      "Validation loss decreased from 2.25325 to 2.175184\n",
      "Current Epoch: 7\n",
      "Training Loss: 2.0709\n",
      "Validation Loss: 1.822663\n",
      "Validation loss decreased from 2.175184 to 1.822663\n",
      "Current Epoch: 8\n",
      "Training Loss: 1.649778\n",
      "Validation Loss: 1.361858\n",
      "Validation loss decreased from 1.822663 to 1.361858\n",
      "Current Epoch: 9\n",
      "Training Loss: 1.378867\n",
      "Validation Loss: 1.171134\n",
      "Validation loss decreased from 1.361858 to 1.171134\n",
      "Current Epoch: 10\n",
      "Training Loss: 1.241648\n",
      "Validation Loss: 1.061616\n",
      "Validation loss decreased from 1.171134 to 1.061616\n",
      "Current Epoch: 11\n",
      "Training Loss: 1.129978\n",
      "Validation Loss: 0.968653\n",
      "Validation loss decreased from 1.061616 to 0.968653\n",
      "Current Epoch: 12\n",
      "Training Loss: 1.039131\n",
      "Validation Loss: 0.876486\n",
      "Validation loss decreased from 0.968653 to 0.876486\n",
      "Current Epoch: 13\n",
      "Training Loss: 0.95312\n",
      "Validation Loss: 0.804498\n",
      "Validation loss decreased from 0.876486 to 0.804498\n",
      "Current Epoch: 14\n",
      "Training Loss: 0.879596\n",
      "Validation Loss: 0.743189\n",
      "Validation loss decreased from 0.804498 to 0.743189\n",
      "Current Epoch: 15\n",
      "Training Loss: 0.824151\n",
      "Validation Loss: 0.699298\n",
      "Validation loss decreased from 0.743189 to 0.699298\n",
      "Current Epoch: 16\n",
      "Training Loss: 0.781424\n",
      "Validation Loss: 0.669614\n",
      "Validation loss decreased from 0.699298 to 0.669614\n",
      "Current Epoch: 17\n",
      "Training Loss: 0.756909\n",
      "Validation Loss: 0.65518\n",
      "Validation loss decreased from 0.669614 to 0.65518\n",
      "Current Epoch: 18\n",
      "Training Loss: 0.729239\n",
      "Validation Loss: 0.630462\n",
      "Validation loss decreased from 0.65518 to 0.630462\n",
      "Current Epoch: 19\n",
      "Training Loss: 0.706706\n",
      "Validation Loss: 0.616319\n",
      "Validation loss decreased from 0.630462 to 0.616319\n",
      "Current Epoch: 20\n",
      "Training Loss: 0.693982\n",
      "Validation Loss: 0.602144\n",
      "Validation loss decreased from 0.616319 to 0.602144\n",
      "Current Epoch: 21\n",
      "Training Loss: 0.68086\n",
      "Validation Loss: 0.594013\n",
      "Validation loss decreased from 0.602144 to 0.594013\n",
      "Current Epoch: 22\n",
      "Training Loss: 0.663896\n",
      "Validation Loss: 0.580354\n",
      "Validation loss decreased from 0.594013 to 0.580354\n",
      "Current Epoch: 23\n",
      "Training Loss: 0.652102\n",
      "Validation Loss: 0.572904\n",
      "Validation loss decreased from 0.580354 to 0.572904\n",
      "Current Epoch: 24\n",
      "Training Loss: 0.642563\n",
      "Validation Loss: 0.568838\n",
      "Validation loss decreased from 0.572904 to 0.568838\n",
      "Current Epoch: 25\n",
      "Training Loss: 0.629519\n",
      "Validation Loss: 0.557695\n",
      "Validation loss decreased from 0.568838 to 0.557695\n"
     ]
    }
   ],
   "source": [
    "# epochs to train for\n",
    "epochs = 25\n",
    "\n",
    "# tracks validation loss change after each epoch\n",
    "minimum_validation_loss = np.inf \n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    # training steps\n",
    "    model.train()\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        # moves tensors to GPU\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # clears gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # loss in batch\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass for loss gradient\n",
    "        loss.backward()\n",
    "        # update paremeters\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # validation steps\n",
    "    model.eval()\n",
    "    for batch_index, (data, target) in enumerate(valid_loader):\n",
    "        # moves tensors to GPU\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # loss in batch\n",
    "        loss = criterion(output, target)\n",
    "        # update validation loss\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # average loss calculations\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    # Display loss statistics\n",
    "    print(f'Current Epoch: {epoch}\\nTraining Loss: {round(train_loss, 6)}\\nValidation Loss: {round(valid_loss, 6)}')\n",
    "\n",
    "    # Saving model every time validation loss decreases\n",
    "    if valid_loss <= minimum_validation_loss:\n",
    "        print(f'Validation loss decreased from {round(minimum_validation_loss, 6)} to {round(valid_loss, 6)}')\n",
    "        torch.save(model.state_dict(), 'trained_model.pt')\n",
    "        minimum_validation_loss = valid_loss\n",
    "        print('Saving New Model')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.556627\n",
      "Test Accuracy of T-shirt/top: 83.09%\n",
      "Test Accuracy of Trouser: 94.64%\n",
      "Test Accuracy of Pullover: 68.33%\n",
      "Test Accuracy of Dress: 85.62%\n",
      "Test Accuracy of Coat: 70.78%\n",
      "Test Accuracy of Sandal: 81.63%\n",
      "Test Accuracy of Shirt: 23.72%\n",
      "Test Accuracy of Sneaker: 91.83%\n",
      "Test Accuracy of Bag: 92.39%\n",
      "Test Accuracy of Ankle boot: 94.04%\n",
      "Full Test Accuracy: 78.58% 6117.0 out of 7784.0\n"
     ]
    }
   ],
   "source": [
    "# tracking test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    # move tensors to GPU\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    # batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # test loss update\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(28):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print(f'Test Loss: {round(test_loss, 6)}')\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print(f'Test Accuracy of {classes[i]}: {round(100*class_correct[i]/class_total[i], 2)}%')\n",
    "    else:\n",
    "        print(f'Test Accuracy of {classes[i]}s: N/A (no training examples)')\n",
    "        \n",
    "        \n",
    "print(f'Full Test Accuracy: {round(100. * np.sum(class_correct) / np.sum(class_total), 2)}% {np.sum(class_correct)} out of {np.sum(class_total)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in Progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
